# Express server
PORT=8080
ALLOW_ORIGIN=*

# llama.cpp model config
# Path to a local GGUF model file. Example below assumes a models/ folder inside backend_express.
LLM_MODEL_PATH=./models/llama-3.2-3b-instruct.Q4_K_M.gguf
LLM_CTX_SIZE=2048
LLM_GPU_LAYERS=0
